{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train, test, fraction = 0):\n",
    "    train_data = pd.read_csv(train + '.csv')\n",
    "    test_data = pd.read_csv(test + '.csv')\n",
    "    if fraction != 0:\n",
    "        train_data = train_data.sample(frac=fraction)\n",
    "        test_data = test_data.sample(frac=fraction)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 0.5% for run-time purposes.\n",
    "train, test = read_data('train', 'test', 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/pretrained_models.html\n",
    "def preprocess():\n",
    "    return BertTokenizer.from_pretrained('bert-base-uncased', additional_special_tokens=['<END_TITLE>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer text data to vector series for training purposes.\n",
    "def encode_func(dataframe):\n",
    "    question_title = dataframe['question_title']\n",
    "    question_body = dataframe['question_body']\n",
    "    answer = dataframe['answer']\n",
    "\n",
    "    question_encoded_dict = preprocess().encode_plus(question_title + ' <END_TITLE> ' + question_body,\n",
    "                                                     None,\n",
    "                                                     max_length=450,\n",
    "                                                     pad_to_max_length=True,\n",
    "                                                     add_special_tokens=True)\n",
    "\n",
    "    answer_encoded_dict = preprocess().encode_plus(answer,\n",
    "                                                   None,\n",
    "                                                   max_length=450,\n",
    "                                                   pad_to_max_length=True,\n",
    "                                                   add_special_tokens=True)\n",
    "\n",
    "    return pd.Series([question_encoded_dict['input_ids'],\n",
    "                      question_encoded_dict['attention_mask'],\n",
    "                      question_encoded_dict['token_type_ids'],\n",
    "                      answer_encoded_dict['input_ids'],\n",
    "                      answer_encoded_dict['attention_mask'],\n",
    "                      answer_encoded_dict['token_type_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(data):\n",
    "    returnme = data[['qa_id']].copy()\n",
    "    returnme[['q_enc',\n",
    "              'q_mask',\n",
    "              'q_type_ids',\n",
    "              'a_enc',\n",
    "              'a_mask',\n",
    "              'a_type_ids']] = data.apply(encode_func,\n",
    "                                          axis=1)\n",
    "    return returnme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = get_token(train)\n",
    "test_tok = get_token(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>q_enc</th>\n",
       "      <th>q_mask</th>\n",
       "      <th>q_type_ids</th>\n",
       "      <th>a_enc</th>\n",
       "      <th>a_mask</th>\n",
       "      <th>a_type_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4898</th>\n",
       "      <td>7795</td>\n",
       "      <td>[101, 16487, 2013, 10200, 4654, 8586, 2000, 24...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[101, 2005, 2005, 2216, 6603, 2054, 1037, 1376...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>7788</td>\n",
       "      <td>[101, 2478, 1037, 8301, 10412, 2005, 3467, 288...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[101, 13433, 2278, 2036, 3084, 1037, 10412, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>4905</td>\n",
       "      <td>[101, 2054, 2515, 1523, 10930, 1011, 7570, 101...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[101, 10930, 1011, 7570, 1011, 7570, 2003, 314...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>7725</td>\n",
       "      <td>[101, 2129, 2064, 1045, 4638, 18833, 3298, 841...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[101, 2008, 1005, 1055, 1037, 2204, 2801, 1998...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>6030</td>\n",
       "      <td>[101, 6358, 3527, 21318, 8370, 3769, 6279, 208...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[101, 1999, 18315, 11022, 3087, 2842, 5927, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qa_id                                              q_enc  \\\n",
       "4898   7795  [101, 16487, 2013, 10200, 4654, 8586, 2000, 24...   \n",
       "4893   7788  [101, 2478, 1037, 8301, 10412, 2005, 3467, 288...   \n",
       "3081   4905  [101, 2054, 2515, 1523, 10930, 1011, 7570, 101...   \n",
       "4855   7725  [101, 2129, 2064, 1045, 4638, 18833, 3298, 841...   \n",
       "3793   6030  [101, 6358, 3527, 21318, 8370, 3769, 6279, 208...   \n",
       "\n",
       "                                                 q_mask  \\\n",
       "4898  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4893  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3081  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4855  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3793  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                             q_type_ids  \\\n",
       "4898  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4893  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3081  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4855  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3793  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                  a_enc  \\\n",
       "4898  [101, 2005, 2005, 2216, 6603, 2054, 1037, 1376...   \n",
       "4893  [101, 13433, 2278, 2036, 3084, 1037, 10412, 20...   \n",
       "3081  [101, 10930, 1011, 7570, 1011, 7570, 2003, 314...   \n",
       "4855  [101, 2008, 1005, 1055, 1037, 2204, 2801, 1998...   \n",
       "3793  [101, 1999, 18315, 11022, 3087, 2842, 5927, 20...   \n",
       "\n",
       "                                                 a_mask  \\\n",
       "4898  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4893  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3081  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4855  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3793  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                             a_type_ids  \n",
       "4898  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4893  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3081  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4855  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3793  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_spit(data, type = 'target'):\n",
    "    column = data.columns[11 : 41]\n",
    "    if type == ('question'):\n",
    "        return column[0 : 21]\n",
    "    else:\n",
    "        return column[21 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = train.columns[11 : 41]\n",
    "question_score = column_spit(train, 'question')\n",
    "answer_score = column_spit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-train stage: pre-process and clean data and change type to int32, this is to increase the speed \n",
    "# of training and also save the memory (64 bit to 32 bit) since our data does not need 64 bit.\n",
    "def pre_train():\n",
    "    \n",
    "    #maxinum length of sentence\n",
    "    leng = 450\n",
    "    #set config to false\n",
    "    config = BertConfig()\n",
    "    #For right now, use no hidden states (runtime)\n",
    "    config.output_hidden_states = False\n",
    "    \n",
    "    #call model\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "    \n",
    "    #call encode, mask, type_ids in int32 format of keras tensorflow layers\n",
    "    ecocoded = tf.keras.layers.Input((leng,), dtype=tf.int32)\n",
    "    \n",
    "    mask = tf.keras.layers.Input((leng,), dtype=tf.int32)\n",
    "    \n",
    "    type_ids = tf.keras.layers.Input((leng,), dtype=tf.int32)\n",
    "    \n",
    "    bert = bert_model(ecocoded, attention_mask=mask, token_type_ids=type_ids)[0]\n",
    "    \n",
    "    bert_summary = tf.keras.layers.Flatten()(tf.keras.layers.AveragePooling1D(leng)(bert))\n",
    "    \n",
    "    return bert_model, ecocoded, mask, type_ids, bert, bert_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_tok.index.isin(train_tok.iloc[train_index].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True, False,  True])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28050808121642fabe2eea0f9c01c83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on 24 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24/24 - 135s - loss: 0.6395\n",
      "Epoch 2/3\n",
      "24/24 - 98s - loss: 0.4933\n",
      "Epoch 3/3\n",
      "24/24 - 103s - loss: 0.4404\n",
      "Train on 24 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24/24 - 167s - loss: 0.6381\n",
      "Epoch 2/3\n",
      "24/24 - 139s - loss: 0.4950\n",
      "Epoch 3/3\n",
      "24/24 - 118s - loss: 0.4339\n",
      "Train on 24 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24/24 - 149s - loss: 0.6260\n",
      "Epoch 2/3\n",
      "24/24 - 138s - loss: 0.4707\n",
      "Epoch 3/3\n",
      "24/24 - 153s - loss: 0.4161\n",
      "Train on 24 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24/24 - 164s - loss: 0.6568\n",
      "Epoch 2/3\n",
      "24/24 - 145s - loss: 0.4831\n",
      "Epoch 3/3\n",
      "24/24 - 1180s - loss: 0.4238\n",
      "Train on 24 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0', 'tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "24/24 - 183s - loss: 0.6196\n",
      "Epoch 2/3\n",
      "24/24 - 164s - loss: 0.4690\n",
      "Epoch 3/3\n",
      "24/24 - 164s - loss: 0.4201\n"
     ]
    }
   ],
   "source": [
    "#tuning parameter\n",
    "k_m = GroupKFold(5)\n",
    "\n",
    "for i, (train_index,\n",
    "        test_index) in enumerate(k_m.split(train_tok,\n",
    "                                           groups=train['question_title'])):\n",
    "\n",
    "    train_boolean = train_tok.index.isin(train_tok.iloc[train_index].index)\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "\n",
    "    question_bert_model, question_encoded, question_mask, question_type_ids, question_bert, question_bert_summary = pre_train()\n",
    "\n",
    "    answer_bert_model, answer_encoded, answer_mask, answer_type_ids, answer_bert, answer_bert_summary = pre_train()\n",
    "\n",
    "    bert_summary = tf.keras.layers.Concatenate()([question_bert_summary, answer_bert_summary])\n",
    "\n",
    "    output = tf.keras.layers.Dense(30, activation='sigmoid')(tf.keras.layers.Dropout(0.2)(bert_summary))\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[question_encoded,\n",
    "                                          question_mask,\n",
    "                                          question_type_ids,\n",
    "                                          answer_encoded,\n",
    "                                          answer_mask,\n",
    "                                          answer_type_ids],\n",
    "                                  outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "                  loss='binary_crossentropy')\n",
    "    model.fit([np.array(list(train_tok.loc[train_boolean, c].values)) for c in\n",
    "               ['q_enc',\n",
    "                'q_mask',\n",
    "                'q_type_ids',\n",
    "                'a_enc',\n",
    "                'a_mask',\n",
    "                'a_type_ids']],\n",
    "              train.loc[train_boolean,\n",
    "                        column].values,\n",
    "              epochs=3,\n",
    "              verbose=2,\n",
    "              batch_size=6)\n",
    "\n",
    "    if not os.path.exists('model_{}'.format(i)):\n",
    "        os.mkdir('model_{}'.format(i))\n",
    "    model.save_weights(os.path.join('model_{}'.format(i),\n",
    "                                    'model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
